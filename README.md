                                                         INFOS ET SITES UTILES 
                                                         
    To DO LIST:
    * improve: -Regulazers (dropouts)
               - Softmax
               -Kernel initializers (he_normal, see Keras.io)  
               -argument de L2 proportionnel a scale loss
    ROOT:                                                   
    * Explication complete histogrammes root : https://root.cern.ch/root/htmldoc/guides/users-guide/Histograms.html
      
    Wiki LIP:
    * https://wiki-lip.lip.pt/Computing/LIP_Lisbon_Farm
    
    NN:
    * Optimisation 
      https://arxiv.org/ftp/arxiv/papers/1808/1808.05979.pdf
      https://www.dlology.com/blog/one-simple-trick-to-train-keras-model-faster-with-batch-normalization/
    * Site de Giles Strong NN: https://amva4newphysics.wordpress.com/tag/neural-networks/
    * Un livre sur le Machine Learning: http://www.deeplearningbook.org/
    * Les amphi de Standfort sur le Machine Learning: https://www.youtube.com/watch?v=NfnWJUyUJYU&list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC
                             et leurs notes de cours : http://cs231n.github.io/
                             et le GitHub: https://github.com/cs231n/cs231n.github.io
    *Plot with keras: https://keras.io/visualization
          with mathplotlib: https://matplotlib.org/3.1.1/tutorials/index.html#introductory
    *Fropout et autres trucs utiles : https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/
                                      http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf
                             
    Questions
    https://www.youtube.com/watch?v=Ql8QPcp8818
    
                             
      
