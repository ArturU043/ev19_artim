## **List of Models and details**

**Model 1** : Just tests / not relevant 
                                
                                                 
**Model 2 : exercise Step 1** ,   Neuros / Layers : 12 ;14 ;1 
                                Activation ReLu ; Output Sigmoid 
                                Batch size : 3000 ; Epochs  100 ; Step size :0.001
                                Optimizer :Adam ; Regularizer : NONE 


 **Model_Ver_3** : Neuron-Layers (12 20 6 3 1) ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM = 2.6970...
 
 
 **Model_Ver_4** : Neuron-Layers (12 24 18 12 6 1) ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM = 2.66452457325
 

 **Model_Ver_5** : Neuron-Layers 12 12 12 12 12 12 12 12 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0

 
 **Model_Ver_6** : Neuron-Layers 12 12 12 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0 

 
 **Model_Ver_7** : Neuron-Layers 12 24 18 12 6 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.9133180197 
 
 
 **Model_Ver_8** : Neuron-Layers 12 60 45 24 60 30 12 50 30 24 10 6 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 500 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.88839993314 
 
 
 **Model_Ver_9** : Neuron-Layers 12 24 18 12 18 12 6 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 150 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.92339752533 
 

 
 **Model_Ver_10** : Neuron-Layers 12 24 18 12 6 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.33655734622 ; Weight Initializer  
 
#Unusable:
 **Model_Ver_11_a** : Neuron-Layers 12 24 18 12 6 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.005 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.65566637017 ; Weight Initializer he_normal 
 
#Unusable:
 **Model_Ver_11_b** : Neuron-Layers 12 24 18 12 6 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.015 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.77734074413 ; Weight Initializer he_normal

 
 **Model_Ver_12** : Neuron-Layers 12 24 18 12 6 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.008 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.76221376486 ; Weight Initializer he_normal 
 

 **Model_Ver_13** : Neuron-Layers 12 24 18 12 6 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.011 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.84142  ; Weight Initializer he_normal 

 
 **Model_Ver_14** : Neuron-Layers: 12 32 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.94432632501 ; Weight Initializer: he_normal 
 
 
  **Model_Ver_15** : Neuron-Layers: 12 24 20 16 12 8 4 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.18657375486 ; Weight Initializer: he_normal 
 

 #Unsuable
 **Model_Ver_16** : Neuron-Layers: 12 24 22 20 18 16 14 12 10 8 6 4 2 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.9377741166 ; Weight Initializer: he_normal 
 
 
 **Model_Ver_16** : Neuron-Layers: 12 24 20 10 6  1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.02432734422 ; Weight Initializer: he_normal 
 
 
 **Model_Ver_17** : Neuron-Layers: 12 24 22 20 10 6  1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.88681790674 ; Weight Initializer: he_normal 


 **Model_Ver_18** : Neuron-Layers: 12 24 20 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.85141820173 ; Weight Initializer: he_normal 
  
 
 **Model_Ver_20** : Neuron-Layers: 12 10 5 2 5 10 5 2 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 0.254887135176 ; Weight Initializer: he_normal
 
 
 **Model_Ver_19** : Neuron-Layers: 12 13 13 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.65594202374 ; Weight Initializer: glorot_uniform 
 
 
 **Model_Ver_21** : Neuron-Layers: 12 24 18 12 4 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.91089838984 ; Weight Initializer: he_normal 
 
 
 **Model_Ver_22** : Neuron-Layers: 12 50 10 2 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.88496242542 ; Weight Initializer: he_normal 
 
 
 **Model_Ver_24** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.85339104882 ; Weight Initializer: he_normal 
 
 
