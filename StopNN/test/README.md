## **List of Models and details**

**Model 1** : Just tests / not relevant 
                                
                                                 
**Model 2 : exercise Step 1** ,   Neuros / Layers : 12 ;14 ;1 
                                Activation ReLu ; Output Sigmoid 
                                Batch size : 3000 ; Epochs  100 ; Step size :0.001
                                Optimizer :Adam ; Regularizer : NONE 


 **Model_Ver_3** : Neuron-Layers (12 20 6 3 1) ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM = 2.6970...
 
 
 **Model_Ver_4** : Neuron-Layers (12 24 18 12 6 1) ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM = 2.66452457325
 
 
 **Model_Ver_4.01** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.56161199895 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_4.02** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.76623312536 ; Weight Initializer: glorot_uniform   
 
 
 
 **Model_Ver_4.04** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.7348105384 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_5** : Neuron-Layers 12 12 12 12 12 12 12 12 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0

 
 **Model_Ver_6** : Neuron-Layers 12 12 12 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0 

 
 **Model_Ver_7** : Neuron-Layers 12 24 18 12 6 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.9133180197 
 
 
 **Model_Ver_7.01** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.86932894753 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_7.02** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.98114550922 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_7.03** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.91913779879 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_7.04** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.05364069279 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_7.05** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.9237763444 ; Weight Initializer: glorot_uniform
 
 
 **Model_Ver_8** : Neuron-Layers 12 60 45 24 60 30 12 50 30 24 10 6 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 500 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.88839993314 
 
 
 **Model_Ver_9** : Neuron-Layers 12 24 18 12 18 12 6 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 150 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.92339752533 
 
  
 **Model_Ver_9.01** : Neuron-Layers: 12 24 18 12 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.17181052143 ; Weight Initializer: he_normal 
 
 
 **Model_Ver_9.02** : Neuron-Layers: 12 24 18 12 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.97568258916 ; Weight Initializer: he_normal   
 

 **Model_Ver_10** : Neuron-Layers 12 24 18 12 6 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.33655734622 ; Weight Initializer  
 
 
  **Model_Ver_10.01** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.85339104882 ; Weight Initializer: he_normal 
  
  
  **Model_Ver_10.02** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.79405051245 ; Weight Initializer: he_normal 

 
 **Model_Ver_10.03** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.79544222588 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_10.04** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.84081642991 ; Weight Initializer: he_normal   

 
  **Model_Ver_10.b1** : Neuron-Layers: 12 24 28 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.64249107323 ; Weight Initializer: glorot_uniform (Full background)
 
 
 **Model_Ver_10.b2** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.5741633872 ; Weight Initializer: glorot_uniform (Full background)  

#Wrong architecture!
 **Model_Ver_10.2b01** : Neuron-Layers: 12 24 18 12 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.40835121836 ; Weight Initializer: he_normal (2 background)  
 
 
**Model_Ver_10.2b02** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.54032468105 ; Weight Initializer: he_normal (2 background)  

#Unusable:
 **Model_Ver_11_a** : Neuron-Layers 12 24 18 12 6 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.005 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.65566637017 ; Weight Initializer he_normal 
 
 
 **Model_Ver_11_a.01** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.005 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.68225868897 ; Weight Initializer: he_normal 
 
 
 **Model_Ver_11_a.02** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.005 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.78858017599 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_11_a.03** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.005 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.9908593048 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_11_a.04** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.005 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.71986152065 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_11_a.05** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.005 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.86095534019 ; Weight Initializer: he_normal   
 
#Unusable:
 **Model_Ver_11_b** : Neuron-Layers 12 24 18 12 6 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.015 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.77734074413 ; Weight Initializer he_normal
 
 
 **Model_Ver_11_b.01** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.015 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.86807410985 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_11_b.02** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.015 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.78320003505 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_11_b.03** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.015 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.83787231651 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_11_b.04** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.015 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.78650900096 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_11_b.05** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.015 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.86841050168 ; Weight Initializer: he_normal   

 
 **Model_Ver_12** : Neuron-Layers 12 24 18 12 6 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.008 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.76221376486 ; Weight Initializer he_normal 
 
  
 **Model_Ver_12.01** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.008 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.03663947437 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_12.02** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.008 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.84545274688 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_12.03** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.008 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.74935643805 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_12.04** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.008 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.94833668012 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_13** : Neuron-Layers 12 24 18 12 6 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.011 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.84142  ; Weight Initializer he_normal 

 
 **Model_Ver_13.01** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.011 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.98034632086 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_13.02** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.011 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.97529850633 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_13.03** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.011 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.14275613432 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_13.04** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.011 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.82938393701 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_14** : Neuron-Layers: 12 32 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.94432632501 ; Weight Initializer: he_normal 
 
 
  **Model_Ver_15** : Neuron-Layers: 12 24 20 16 12 8 4 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.18657375486 ; Weight Initializer: he_normal 
 

 #Unsuable
 **Model_Ver_16** : Neuron-Layers: 12 24 22 20 18 16 14 12 10 8 6 4 2 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.9377741166 ; Weight Initializer: he_normal 
 
 
 **Model_Ver_16** : Neuron-Layers: 12 24 20 10 6  1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.02432734422 ; Weight Initializer: he_normal 
 
 
 **Model_Ver_17** : Neuron-Layers: 12 24 22 20 10 6  1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.88681790674 ; Weight Initializer: he_normal 


 **Model_Ver_17.01** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.03927948393 ; Weight Initializer: he_normal     
 
 
 **Model_Ver_17.02** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.0769923092 ; Weight Initializer: he_normal   
  

**Model_Ver_17.03** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.95593709999 ; Weight Initializer: he_normal   


 **Model_Ver_17.04** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.11375711475 ; Weight Initializer: he_normal   
 

 **Model_Ver_17.b1** : Neuron-Layers: 12 24 22 20 10  6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.80447116613 ; Weight Initializer: glorot_uniform (Full background)  
 
 
 **Model_Ver_18** : Neuron-Layers: 12 24 20 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.85141820173 ; Weight Initializer: he_normal 
  
  
 **Model_Ver_19** : Neuron-Layers: 12 13 13 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.65594202374 ; Weight Initializer: glorot_uniform 


 **Model_Ver_19.01** : Neuron-Layers: 12 13 13 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.01199317939 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_19.02** : Neuron-Layers: 12 13 13 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.78968758717 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_19.03** : Neuron-Layers: 12 13 13 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.63576918929 ; Weight Initializer: glorot_uniform   
 
 
  **Model_Ver_19.04** : Neuron-Layers: 12 13 13 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.68733973864 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_19.05** : Neuron-Layers: 12 13 13 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.003 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.67355940383 ; Weight Initializer: glorot_uniform   
 
 
 **Model_Ver_20** : Neuron-Layers: 12 10 5 2 5 10 5 2 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 0.254887135176 ; Weight Initializer: he_normal
 
 
 **Model_Ver_21** : Neuron-Layers: 12 24 18 12 4 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.91089838984 ; Weight Initializer: he_normal 
 
 
 **Model_Ver_22** : Neuron-Layers: 12 50 10 2 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.88496242542 ; Weight Initializer: he_normal 
 

 **Model_Ver_23.01** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.59776078108 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_23.02** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.57875218889 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_23.03** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.55928529227 ; Weight Initializer: he_normal  

 
 **Model_Ver_23.04** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.76469540903 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_23.05** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.60293930743 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_24.01** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.85633085129 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_24.02** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.74335988393 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_24.03** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.60135496805 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_24.04** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.61992182245 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_24.05** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.51038333089 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_25.01** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.008 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.07032119461 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_25.02** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.008 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.89885588378 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_25.03** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.008 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.89604675021 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_25.04** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.008 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.84942603491 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_25.05** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.008 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.99464227199 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_26.01** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.011 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.8650328365 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_26.03** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.011 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.0731219564 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_26.04** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.011 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.1523509458 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_26.05** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.011 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.27362565387 ; Weight Initializer: he_normal   
 

 **Model_Ver_27.01** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.015 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.87150423672 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_27.02** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.015 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.18220884667 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_27.03** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.015 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.92401723522 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_27.04** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.015 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.84972056078 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_27.05** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.015 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.01894783239 ; Weight Initializer: he_normal 
 
   
   **Model_Ver_28.01** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.005 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.88231387017 ; Weight Initializer: he_normal   
   
   
 **Model_Ver_28.03** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.015 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.89404280603 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_28.04** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.005 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.97898289918 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_28.05** : Neuron-Layers: 12 24 22 20 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.015 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.16162550664 ; Weight Initializer: he_normal   
 
 
  **Model_Ver_29** : Neuron-Layers: 12 32 24 16 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.70959951273 ; Weight Initializer: he_normal   
  
 
  **Model_Ver_29.01** : Neuron-Layers: 12 32 24 16 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.88076024203 ; Weight Initializer: he_normal   
  
 
 **Model_Ver_29.02** : Neuron-Layers: 12 32 24 16 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.79249262617 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_29.03** : Neuron-Layers: 12 32 24 16 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.26318539086 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_29.04** : Neuron-Layers: 12 32 24 16 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.92015659069 ; Weight Initializer: he_normal   
 
  
 **Model_Ver_30.01** : Neuron-Layers: 12 32 24 16 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.015 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.77721361921 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_30.03** : Neuron-Layers: 12 32 24 16 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.015 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.93970313335 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_30.04** : Neuron-Layers: 12 32 24 16 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.015 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.00563810197 ; Weight Initializer: he_normal   
 
 
