## **List of Models and details**

**Model 1** : Just tests / not relevant 
                                
                                                 
**Model 2 : exercise Step 1** ,   Neuros / Layers : 12 ;14 ;1 
                                Activation ReLu ; Output Sigmoid 
                                Batch size : 3000 ; Epochs  100 ; Step size :0.001
                                Optimizer :Adam ; Regularizer : NONE 


 **Model_Ver_3** : Neuron-Layers (12 20 6 3 1) ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM = 2.6970...
 
 
 **Model_Ver_4** : Neuron-Layers (12 24 18 12 6 1) ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM = 2.66452457325
 

 **Model_Ver_5** : Neuron-Layers 12 12 12 12 12 12 12 12 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0

 
 **Model_Ver_6** : Neuron-Layers 12 12 12 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0 

 
 **Model_Ver_7** : Neuron-Layers 12 24 18 12 6 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.9133180197 
 
 
 **Model_Ver_8** : Neuron-Layers 12 60 45 24 60 30 12 50 30 24 10 6 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 500 ; Step size: 0.001 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.88839993314 
 
 
 **Model_Ver_9** : Neuron-Layers 12 24 18 12 18 12 6 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 150 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.92339752533 
 

 
 **Model_Ver_10** : Neuron-Layers 12 24 18 12 6 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.33655734622 ; Weight Initializer  
 
 
 **Model_Ver_11** : Neuron-Layers 12 24 18 12 6 1 ; Activation: ReLu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.005 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.65566637017 ; Weight Initializer he_normal 
 