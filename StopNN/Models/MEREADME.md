
 
 **Model_Ver_31.01** : Neuron-Layers: 12 32 24 16 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.005 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.96724094828 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_31.02** : Neuron-Layers: 12 32 24 16 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.005 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.0359708992 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_31.03** : Neuron-Layers: 12 32 24 16 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.005 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.10688471849 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_31.04** : Neuron-Layers: 12 32 24 16 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.005 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 3.00434633032 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_31.05** : Neuron-Layers: 12 32 24 16 10 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.005 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.95183229709 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_10.11** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 25.0912181973 ; Weight Initializer: he_normal   
 
 
 **Model_Ver_10.12** : Neuron-Layers: 12 24 18 12 6 1 ; Activation: relu ; Output: Sigmoid ; Batch size:3000 ; Epochs: 100 ; Step size: 0.01 ; Optimizer: Adam ; Regulizer: 0 ; Max FOM : 2.7621901865 ; Weight Initializer: he_normal   
 